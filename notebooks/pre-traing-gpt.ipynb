{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":1690352,"sourceType":"datasetVersion","datasetId":1001669}],"dockerImageVersionId":30747,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!git clone https://github.com/MorningStarTM/large-language-model-creation.git","metadata":{"execution":{"iopub.status.busy":"2024-07-25T14:07:57.273931Z","iopub.execute_input":"2024-07-25T14:07:57.274308Z","iopub.status.idle":"2024-07-25T14:07:58.812208Z","shell.execute_reply.started":"2024-07-25T14:07:57.274277Z","shell.execute_reply":"2024-07-25T14:07:58.811246Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"Cloning into 'large-language-model-creation'...\nremote: Enumerating objects: 41, done.\u001b[K\nremote: Counting objects: 100% (41/41), done.\u001b[K\nremote: Compressing objects: 100% (25/25), done.\u001b[K\nremote: Total 41 (delta 11), reused 35 (delta 9), pack-reused 0\u001b[K\nUnpacking objects: 100% (41/41), 9.68 KiB | 991.00 KiB/s, done.\n","output_type":"stream"}]},{"cell_type":"code","source":"%cd /kaggle/working/large-language-model-creation","metadata":{"execution":{"iopub.status.busy":"2024-07-25T14:08:00.743335Z","iopub.execute_input":"2024-07-25T14:08:00.744140Z","iopub.status.idle":"2024-07-25T14:08:00.751140Z","shell.execute_reply.started":"2024-07-25T14:08:00.744102Z","shell.execute_reply":"2024-07-25T14:08:00.750238Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stdout","text":"/kaggle/working/large-language-model-creation\n","output_type":"stream"}]},{"cell_type":"code","source":"!ls","metadata":{"execution":{"iopub.status.busy":"2024-07-25T14:08:02.095291Z","iopub.execute_input":"2024-07-25T14:08:02.095670Z","iopub.status.idle":"2024-07-25T14:08:03.095726Z","shell.execute_reply.started":"2024-07-25T14:08:02.095638Z","shell.execute_reply":"2024-07-25T14:08:03.094767Z"},"trusted":true},"execution_count":3,"outputs":[{"name":"stdout","text":"LICENSE  README.md  models  notebooks  tokenizer\n","output_type":"stream"}]},{"cell_type":"code","source":"import os\nimport re\nimport json\nfrom models import GPTLanguageModel\nfrom tokenizer import WordLevelTokenizer\nimport torch","metadata":{"execution":{"iopub.status.busy":"2024-07-25T14:08:04.152397Z","iopub.execute_input":"2024-07-25T14:08:04.152786Z","iopub.status.idle":"2024-07-25T14:08:07.609563Z","shell.execute_reply.started":"2024-07-25T14:08:04.152755Z","shell.execute_reply":"2024-07-25T14:08:07.608490Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\ndevice\nmax_iters = 60000\nlearning_rate = 3e-4\nblock_size = 8\nbatch_size = 8\neval_iters = 500\nn_emb = 384\nn_layers = 6\nn_head = 6","metadata":{"execution":{"iopub.status.busy":"2024-07-25T14:10:11.913596Z","iopub.execute_input":"2024-07-25T14:10:11.914481Z","iopub.status.idle":"2024-07-25T14:10:11.919759Z","shell.execute_reply.started":"2024-07-25T14:10:11.914446Z","shell.execute_reply":"2024-07-25T14:10:11.918882Z"},"trusted":true},"execution_count":23,"outputs":[]},{"cell_type":"markdown","source":"# function for get vocab size","metadata":{}},{"cell_type":"code","source":"def get_vocab_size(corpus):\n    \"\"\"\n    Get the vocabulary size of the given corpus.\n\n    Parameters:\n    corpus (str): The text corpus to analyze.\n\n    Returns:\n    int: The size of the vocabulary (number of unique words and punctuation).\n    \"\"\"\n    words = preprocess_text(corpus)\n    unique_words = set(words)\n    return len(unique_words)\n\ndef preprocess_text(text):\n    \"\"\"\n    Preprocess the text by converting to lowercase and splitting into words and punctuation.\n\n    Parameters:\n    text (str): The text to preprocess.\n\n    Returns:\n    list: A list of words and punctuation.\n    \"\"\"\n    text = text.lower()\n    words = re.findall(r'\\b\\w+\\b|[^\\w\\s]', text)\n    return words","metadata":{"execution":{"iopub.status.busy":"2024-07-25T14:10:13.232911Z","iopub.execute_input":"2024-07-25T14:10:13.233656Z","iopub.status.idle":"2024-07-25T14:10:13.239399Z","shell.execute_reply.started":"2024-07-25T14:10:13.233622Z","shell.execute_reply":"2024-07-25T14:10:13.238362Z"},"trusted":true},"execution_count":24,"outputs":[]},{"cell_type":"markdown","source":"# function for read json file","metadata":{}},{"cell_type":"code","source":"def read_json_files(directory_path):\n    all_text = \"\"\n\n    # Get the list of files in the directory\n    files = os.listdir(directory_path)\n\n    # Loop through the first n files in the directory\n    for filename in files[:1]:\n        if filename.endswith(\".json\"):\n            file_path = os.path.join(directory_path, filename)\n\n            # Read the JSON file\n            with open(file_path, 'r', encoding='utf-8') as file:\n                data = json.load(file)\n\n                # Iterate through each object in the array and concatenate the 'text' values\n                for item in data:\n                    if 'text' in item:\n                        all_text += item['text'] + \" \"\n\n    return all_text","metadata":{"execution":{"iopub.status.busy":"2024-07-25T14:10:13.575874Z","iopub.execute_input":"2024-07-25T14:10:13.576658Z","iopub.status.idle":"2024-07-25T14:10:13.583545Z","shell.execute_reply.started":"2024-07-25T14:10:13.576626Z","shell.execute_reply":"2024-07-25T14:10:13.582451Z"},"trusted":true},"execution_count":25,"outputs":[]},{"cell_type":"markdown","source":"# Read data","metadata":{}},{"cell_type":"code","source":"text = read_json_files(\"/kaggle/input/plain-text-wikipedia-202011/enwiki20201020\") ","metadata":{"execution":{"iopub.status.busy":"2024-07-25T14:10:13.992282Z","iopub.execute_input":"2024-07-25T14:10:13.993152Z","iopub.status.idle":"2024-07-25T14:10:14.809627Z","shell.execute_reply.started":"2024-07-25T14:10:13.993118Z","shell.execute_reply":"2024-07-25T14:10:14.808765Z"},"trusted":true},"execution_count":26,"outputs":[]},{"cell_type":"code","source":"text[0:500]","metadata":{"execution":{"iopub.status.busy":"2024-07-25T14:10:14.811354Z","iopub.execute_input":"2024-07-25T14:10:14.811682Z","iopub.status.idle":"2024-07-25T14:10:14.817702Z","shell.execute_reply.started":"2024-07-25T14:10:14.811656Z","shell.execute_reply":"2024-07-25T14:10:14.816714Z"},"trusted":true},"execution_count":27,"outputs":[{"execution_count":27,"output_type":"execute_result","data":{"text/plain":"\"Travis are a Scottish rock band formed in Glasgow in 1990, composed of Fran Healy (lead vocals, rhythm guitar), Dougie Payne (bass guitar, backing vocals), Andy Dunlop (lead guitar, banjo, backing vocals) and Neil Primrose (drums, percussion). The band's name comes from the Harry Dean Stanton character Travis Henderson from the film Paris, Texas. The band released their debut album, Good Feeling (1997), to moderate success where it debuted at number nine on the UK Albums Chart and went onto achi\""},"metadata":{}}]},{"cell_type":"markdown","source":"# Tokenizing","metadata":{}},{"cell_type":"code","source":"tokenizer = WordLevelTokenizer()\ntokenizer.fit(text)","metadata":{"execution":{"iopub.status.busy":"2024-07-25T14:10:14.819004Z","iopub.execute_input":"2024-07-25T14:10:14.819357Z","iopub.status.idle":"2024-07-25T14:10:21.167969Z","shell.execute_reply.started":"2024-07-25T14:10:14.819327Z","shell.execute_reply":"2024-07-25T14:10:21.167020Z"},"trusted":true},"execution_count":28,"outputs":[{"name":"stderr","text":"Building Vocabulary: 100%|██████████| 193460/193460 [00:00<00:00, 754748.44it/s]\n","output_type":"stream"}]},{"cell_type":"code","source":"temp = \"Dunlop (lead guitar, banjo, backing vocals) and Neil Primrose (drums, percussion).\"\n\ntokens = tokenizer.tokenize(temp)\nprint(\"Tokens:\", tokens)  # Output: Tokens: [index values representing each word]\n\noriginal_text = tokenizer.detokenize(tokens)\nprint(\"Detokenized text:\", original_text)","metadata":{"execution":{"iopub.status.busy":"2024-07-25T14:10:21.170939Z","iopub.execute_input":"2024-07-25T14:10:21.171607Z","iopub.status.idle":"2024-07-25T14:10:21.177316Z","shell.execute_reply.started":"2024-07-25T14:10:21.171567Z","shell.execute_reply":"2024-07-25T14:10:21.176364Z"},"trusted":true},"execution_count":29,"outputs":[{"name":"stdout","text":"Tokens: [73292, 44490, 174550, 109226, 169896, 58719, 169896, 139128, 59307, 137107, 13093, 15393, 171967, 44490, 106808, 169896, 140031, 137107, 31079]\nDetokenized text: dunlop ( lead guitar , banjo , backing vocals ) and neil primrose ( drums , percussion ) .\n","output_type":"stream"}]},{"cell_type":"code","source":"vocab_size = get_vocab_size(text)\nprint(\"Vocabulary Size:\", vocab_size)  ","metadata":{"execution":{"iopub.status.busy":"2024-07-25T14:10:21.178788Z","iopub.execute_input":"2024-07-25T14:10:21.179110Z","iopub.status.idle":"2024-07-25T14:10:27.238466Z","shell.execute_reply.started":"2024-07-25T14:10:21.179086Z","shell.execute_reply":"2024-07-25T14:10:27.237503Z"},"trusted":true},"execution_count":30,"outputs":[{"name":"stdout","text":"Vocabulary Size: 193460\n","output_type":"stream"}]},{"cell_type":"code","source":"data = tokenizer.tokenize(text)","metadata":{"execution":{"iopub.status.busy":"2024-07-25T14:10:27.239726Z","iopub.execute_input":"2024-07-25T14:10:27.240032Z","iopub.status.idle":"2024-07-25T14:10:35.341835Z","shell.execute_reply.started":"2024-07-25T14:10:27.240007Z","shell.execute_reply":"2024-07-25T14:10:35.340958Z"},"trusted":true},"execution_count":31,"outputs":[]},{"cell_type":"code","source":"encoded_data = torch.tensor(data, dtype=torch.long)\nprint(encoded_data.shape, encoded_data.dtype)","metadata":{"execution":{"iopub.status.busy":"2024-07-25T14:10:35.342965Z","iopub.execute_input":"2024-07-25T14:10:35.343265Z","iopub.status.idle":"2024-07-25T14:10:36.558908Z","shell.execute_reply.started":"2024-07-25T14:10:35.343240Z","shell.execute_reply":"2024-07-25T14:10:36.557807Z"},"trusted":true},"execution_count":32,"outputs":[{"name":"stdout","text":"torch.Size([8407280]) torch.int64\n","output_type":"stream"}]},{"cell_type":"code","source":"n = int(0.9*len(encoded_data))\ntrain_data = encoded_data[:n]\nval_data = encoded_data[n:]\n\nprint(f\"Training tokens : {len(train_data)}  --- Validation tokens : {len(val_data)}\")","metadata":{"execution":{"iopub.status.busy":"2024-07-25T14:10:36.560372Z","iopub.execute_input":"2024-07-25T14:10:36.560788Z","iopub.status.idle":"2024-07-25T14:10:36.572599Z","shell.execute_reply.started":"2024-07-25T14:10:36.560750Z","shell.execute_reply":"2024-07-25T14:10:36.571706Z"},"trusted":true},"execution_count":33,"outputs":[{"name":"stdout","text":"Training tokens : 7566552  --- Validation tokens : 840728\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# Prepare data for training (given text -> next token)","metadata":{}},{"cell_type":"code","source":"x = train_data[:block_size]\ny = train_data[1:block_size+1]\nfor t in range(block_size):\n    context = x[:t+1]\n    target = y[t]\n    print(f\"when input is {context} the target: {target}\")","metadata":{"execution":{"iopub.status.busy":"2024-07-25T14:10:36.573773Z","iopub.execute_input":"2024-07-25T14:10:36.574054Z","iopub.status.idle":"2024-07-25T14:10:36.591569Z","shell.execute_reply.started":"2024-07-25T14:10:36.574030Z","shell.execute_reply":"2024-07-25T14:10:36.590615Z"},"trusted":true},"execution_count":34,"outputs":[{"name":"stdout","text":"when input is tensor([108528]) the target: 163487\nwhen input is tensor([108528, 163487]) the target: 152830\nwhen input is tensor([108528, 163487, 152830]) the target: 156102\nwhen input is tensor([108528, 163487, 152830, 156102]) the target: 180323\nwhen input is tensor([108528, 163487, 152830, 156102, 180323]) the target: 37988\nwhen input is tensor([108528, 163487, 152830, 156102, 180323,  37988]) the target: 127861\nwhen input is tensor([108528, 163487, 152830, 156102, 180323,  37988, 127861]) the target: 32808\nwhen input is tensor([108528, 163487, 152830, 156102, 180323,  37988, 127861,  32808]) the target: 186457\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# Make Batch","metadata":{}},{"cell_type":"code","source":"torch.manual_seed(1337)\n\ndef get_batch(split):\n    data = train_data if split == \"train\" else val_data\n    ix = torch.randint(len(encoded_data) - block_size, (batch_size,))\n    x = torch.stack([encoded_data[i:i+block_size] for i in ix])\n    y = torch.stack([encoded_data[i+1:i+block_size+1] for i in ix])\n    return x, y\n\nxb, yb = get_batch('train')\nprint(\"Inputs: \")\nprint(xb)\nprint(\"Targets: \")\nprint(yb)","metadata":{"execution":{"iopub.status.busy":"2024-07-25T14:10:36.593799Z","iopub.execute_input":"2024-07-25T14:10:36.594092Z","iopub.status.idle":"2024-07-25T14:10:36.605742Z","shell.execute_reply.started":"2024-07-25T14:10:36.594068Z","shell.execute_reply":"2024-07-25T14:10:36.604762Z"},"trusted":true},"execution_count":35,"outputs":[{"name":"stdout","text":"Inputs: \ntensor([[ 87833,  31248, 178572, 181040,  42148,  87833,  10707,  76995],\n        [ 13093,  17796, 184565, 151654, 158933,  60669,  64900,   1317],\n        [191453,  54683, 122536, 169896, 180645,    772,  18014, 143272],\n        [118272,  61777, 181415, 118272, 118272,   4217, 189779,  22124],\n        [ 26433, 180323, 183916,  34639,  32878, 107152, 192640, 169896],\n        [118272, 149079,  87221, 176181,  46032,  37988,  87221,  87221],\n        [169896,  12297,  31079, 113583,   4217,  68419, 182554, 169896],\n        [ 67347,  13093,  24404, 107152, 146747, 108802, 169896,  75358]])\nTargets: \ntensor([[ 31248, 178572, 181040,  42148,  87833,  10707,  76995,  37988],\n        [ 17796, 184565, 151654, 158933,  60669,  64900,   1317, 190678],\n        [ 54683, 122536, 169896, 180645,    772,  18014, 143272,  91450],\n        [ 61777, 181415, 118272, 118272,   4217, 189779,  22124,  19321],\n        [180323, 183916,  34639,  32878, 107152, 192640, 169896, 190678],\n        [149079,  87221, 176181,  46032,  37988,  87221,  87221, 147228],\n        [ 12297,  31079, 113583,   4217,  68419, 182554, 169896, 105074],\n        [ 13093,  24404, 107152, 146747, 108802, 169896,  75358,  87787]])\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# GPT Model","metadata":{}},{"cell_type":"code","source":"model = GPTLanguageModel(vocab_size=vocab_size)\nmodel.to(device)","metadata":{"execution":{"iopub.status.busy":"2024-07-25T14:09:24.633486Z","iopub.execute_input":"2024-07-25T14:09:24.633869Z","iopub.status.idle":"2024-07-25T14:09:29.146202Z","shell.execute_reply.started":"2024-07-25T14:09:24.633841Z","shell.execute_reply":"2024-07-25T14:09:29.144867Z"},"trusted":true},"execution_count":20,"outputs":[{"execution_count":20,"output_type":"execute_result","data":{"text/plain":"GPTLanguageModel(\n  (token_embeding_table): Embedding(193460, 384)\n  (position_embedding_table): Embedding(193460, 384)\n  (blocks): Sequential(\n    (0): Block(\n      (selfAttention): MultiHeadAttention(\n        (heads): ModuleList(\n          (0-3): 4 x Head(\n            (key): Linear(in_features=384, out_features=96, bias=False)\n            (query): Linear(in_features=384, out_features=96, bias=False)\n            (value): Linear(in_features=384, out_features=96, bias=False)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (proj): Linear(in_features=384, out_features=384, bias=True)\n        (dropout): Dropout(p=0.1, inplace=False)\n      )\n      (ffwd): FeedForward(\n        (net): Sequential(\n          (0): Linear(in_features=384, out_features=1536, bias=True)\n          (1): ReLU()\n          (2): Linear(in_features=1536, out_features=384, bias=True)\n          (3): Dropout(p=0.1, inplace=False)\n        )\n      )\n      (ln1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n      (ln2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n    )\n    (1): Block(\n      (selfAttention): MultiHeadAttention(\n        (heads): ModuleList(\n          (0-3): 4 x Head(\n            (key): Linear(in_features=384, out_features=96, bias=False)\n            (query): Linear(in_features=384, out_features=96, bias=False)\n            (value): Linear(in_features=384, out_features=96, bias=False)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (proj): Linear(in_features=384, out_features=384, bias=True)\n        (dropout): Dropout(p=0.1, inplace=False)\n      )\n      (ffwd): FeedForward(\n        (net): Sequential(\n          (0): Linear(in_features=384, out_features=1536, bias=True)\n          (1): ReLU()\n          (2): Linear(in_features=1536, out_features=384, bias=True)\n          (3): Dropout(p=0.1, inplace=False)\n        )\n      )\n      (ln1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n      (ln2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n    )\n    (2): Block(\n      (selfAttention): MultiHeadAttention(\n        (heads): ModuleList(\n          (0-3): 4 x Head(\n            (key): Linear(in_features=384, out_features=96, bias=False)\n            (query): Linear(in_features=384, out_features=96, bias=False)\n            (value): Linear(in_features=384, out_features=96, bias=False)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (proj): Linear(in_features=384, out_features=384, bias=True)\n        (dropout): Dropout(p=0.1, inplace=False)\n      )\n      (ffwd): FeedForward(\n        (net): Sequential(\n          (0): Linear(in_features=384, out_features=1536, bias=True)\n          (1): ReLU()\n          (2): Linear(in_features=1536, out_features=384, bias=True)\n          (3): Dropout(p=0.1, inplace=False)\n        )\n      )\n      (ln1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n      (ln2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n    )\n    (3): Block(\n      (selfAttention): MultiHeadAttention(\n        (heads): ModuleList(\n          (0-3): 4 x Head(\n            (key): Linear(in_features=384, out_features=96, bias=False)\n            (query): Linear(in_features=384, out_features=96, bias=False)\n            (value): Linear(in_features=384, out_features=96, bias=False)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (proj): Linear(in_features=384, out_features=384, bias=True)\n        (dropout): Dropout(p=0.1, inplace=False)\n      )\n      (ffwd): FeedForward(\n        (net): Sequential(\n          (0): Linear(in_features=384, out_features=1536, bias=True)\n          (1): ReLU()\n          (2): Linear(in_features=1536, out_features=384, bias=True)\n          (3): Dropout(p=0.1, inplace=False)\n        )\n      )\n      (ln1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n      (ln2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n    )\n  )\n  (ln_f): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n  (lm_head): Linear(in_features=384, out_features=193460, bias=True)\n)"},"metadata":{}}]},{"cell_type":"code","source":"@torch.no_grad()\ndef estimate_loss():\n    out = {}\n    model.eval()\n    for split in ['train', 'val']:\n        losses = torch.zeros(eval_iters)\n        for k in range(eval_iters):\n            X, Y = get_batch(split)\n            X, Y = X.to(device), Y.to(device)\n            logits, loss = model(X, Y)\n            losses[k] = loss.item()\n        out[split] = losses.mean()\n    model.train()\n    return out","metadata":{"execution":{"iopub.status.busy":"2024-07-25T14:10:49.402044Z","iopub.execute_input":"2024-07-25T14:10:49.402447Z","iopub.status.idle":"2024-07-25T14:10:49.409398Z","shell.execute_reply.started":"2024-07-25T14:10:49.402416Z","shell.execute_reply":"2024-07-25T14:10:49.408234Z"},"trusted":true},"execution_count":36,"outputs":[]},{"cell_type":"code","source":"optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n\nfor iter in range(max_iters):\n    if iter % eval_iters == 0:\n        losses = estimate_loss()\n        print(f\"steps: {iter} train loss: {losses['train']} val loss: {losses['val']}\")\n        \n    xb, yb = get_batch('train')\n    xb = xb.to(device)\n    yb = yb.to(device)\n\n    logits, loss = model.forward(xb, yb)\n    optimizer.zero_grad(set_to_none=True)\n    loss.backward()\n    optimizer.step()\nprint(loss.item())","metadata":{"execution":{"iopub.status.busy":"2024-07-25T14:10:49.692161Z","iopub.execute_input":"2024-07-25T14:10:49.693052Z","iopub.status.idle":"2024-07-25T15:25:00.994892Z","shell.execute_reply.started":"2024-07-25T14:10:49.693017Z","shell.execute_reply":"2024-07-25T15:25:00.993890Z"},"trusted":true},"execution_count":37,"outputs":[{"name":"stdout","text":"steps: 0 train loss: 12.255699157714844 val loss: 12.252248764038086\nsteps: 500 train loss: 7.22445011138916 val loss: 7.2313995361328125\nsteps: 1000 train loss: 6.939018249511719 val loss: 6.923289775848389\nsteps: 1500 train loss: 6.837271690368652 val loss: 6.78727912902832\nsteps: 2000 train loss: 6.676595211029053 val loss: 6.688830852508545\nsteps: 2500 train loss: 6.644763946533203 val loss: 6.6422576904296875\nsteps: 3000 train loss: 6.503884792327881 val loss: 6.57376766204834\nsteps: 3500 train loss: 6.564083099365234 val loss: 6.543325901031494\nsteps: 4000 train loss: 6.50327205657959 val loss: 6.506964683532715\nsteps: 4500 train loss: 6.4867448806762695 val loss: 6.480050086975098\nsteps: 5000 train loss: 6.445086479187012 val loss: 6.434764862060547\nsteps: 5500 train loss: 6.423729419708252 val loss: 6.474671363830566\nsteps: 6000 train loss: 6.43995475769043 val loss: 6.428317070007324\nsteps: 6500 train loss: 6.359946250915527 val loss: 6.368020057678223\nsteps: 7000 train loss: 6.383557319641113 val loss: 6.368819236755371\nsteps: 7500 train loss: 6.393247127532959 val loss: 6.342820167541504\nsteps: 8000 train loss: 6.335867881774902 val loss: 6.365123748779297\nsteps: 8500 train loss: 6.340653419494629 val loss: 6.292685508728027\nsteps: 9000 train loss: 6.333600997924805 val loss: 6.323637008666992\nsteps: 9500 train loss: 6.287308216094971 val loss: 6.298454284667969\nsteps: 10000 train loss: 6.235362529754639 val loss: 6.263166904449463\nsteps: 10500 train loss: 6.316277027130127 val loss: 6.230998516082764\nsteps: 11000 train loss: 6.2144598960876465 val loss: 6.208511829376221\nsteps: 11500 train loss: 6.200788974761963 val loss: 6.280547618865967\nsteps: 12000 train loss: 6.230501651763916 val loss: 6.279308319091797\nsteps: 12500 train loss: 6.1564226150512695 val loss: 6.161911487579346\nsteps: 13000 train loss: 6.22231912612915 val loss: 6.198886394500732\nsteps: 13500 train loss: 6.2318854331970215 val loss: 6.177519798278809\nsteps: 14000 train loss: 6.2059478759765625 val loss: 6.186082363128662\nsteps: 14500 train loss: 6.206500053405762 val loss: 6.233273983001709\nsteps: 15000 train loss: 6.1236114501953125 val loss: 6.174377918243408\nsteps: 15500 train loss: 6.158488750457764 val loss: 6.1217942237854\nsteps: 16000 train loss: 6.1207122802734375 val loss: 6.122230529785156\nsteps: 16500 train loss: 6.115413188934326 val loss: 6.152519702911377\nsteps: 17000 train loss: 6.1497626304626465 val loss: 6.098599910736084\nsteps: 17500 train loss: 6.093377113342285 val loss: 6.076777935028076\nsteps: 18000 train loss: 6.066067695617676 val loss: 6.090334892272949\nsteps: 18500 train loss: 6.09871768951416 val loss: 6.0253143310546875\nsteps: 19000 train loss: 6.099852561950684 val loss: 6.089484214782715\nsteps: 19500 train loss: 6.0989227294921875 val loss: 6.138276100158691\nsteps: 20000 train loss: 6.053012847900391 val loss: 6.0517754554748535\nsteps: 20500 train loss: 6.0430169105529785 val loss: 5.987912654876709\nsteps: 21000 train loss: 6.0061726570129395 val loss: 6.039829254150391\nsteps: 21500 train loss: 6.085712432861328 val loss: 6.083908557891846\nsteps: 22000 train loss: 6.042364597320557 val loss: 6.032512664794922\nsteps: 22500 train loss: 6.032880783081055 val loss: 6.0613861083984375\nsteps: 23000 train loss: 6.0290656089782715 val loss: 5.999475479125977\nsteps: 23500 train loss: 5.962433338165283 val loss: 5.974575996398926\nsteps: 24000 train loss: 6.034114837646484 val loss: 6.017488956451416\nsteps: 24500 train loss: 5.964020252227783 val loss: 6.011180877685547\nsteps: 25000 train loss: 5.964944362640381 val loss: 6.0126142501831055\nsteps: 25500 train loss: 5.987765789031982 val loss: 5.959946155548096\nsteps: 26000 train loss: 5.986249923706055 val loss: 5.979244232177734\nsteps: 26500 train loss: 6.007048606872559 val loss: 5.998025417327881\nsteps: 27000 train loss: 6.00913667678833 val loss: 5.995882511138916\nsteps: 27500 train loss: 5.965116500854492 val loss: 5.928663730621338\nsteps: 28000 train loss: 5.992269039154053 val loss: 5.99201774597168\nsteps: 28500 train loss: 5.936527729034424 val loss: 5.936990261077881\nsteps: 29000 train loss: 5.989001750946045 val loss: 5.958929061889648\nsteps: 29500 train loss: 5.954122066497803 val loss: 5.974720478057861\nsteps: 30000 train loss: 5.981009006500244 val loss: 5.910815238952637\nsteps: 30500 train loss: 5.91988468170166 val loss: 5.886514663696289\nsteps: 31000 train loss: 5.887033462524414 val loss: 5.948220252990723\nsteps: 31500 train loss: 5.929265022277832 val loss: 5.948951721191406\nsteps: 32000 train loss: 5.915339946746826 val loss: 5.959643363952637\nsteps: 32500 train loss: 5.922828674316406 val loss: 5.970863342285156\nsteps: 33000 train loss: 5.964371204376221 val loss: 5.904940605163574\nsteps: 33500 train loss: 5.918092250823975 val loss: 5.944551944732666\nsteps: 34000 train loss: 5.921146869659424 val loss: 5.942282676696777\nsteps: 34500 train loss: 5.906037330627441 val loss: 5.9521403312683105\nsteps: 35000 train loss: 5.940369129180908 val loss: 5.887274265289307\nsteps: 35500 train loss: 5.917369365692139 val loss: 5.843900680541992\nsteps: 36000 train loss: 5.906949043273926 val loss: 5.904956817626953\nsteps: 36500 train loss: 5.923199653625488 val loss: 5.907980442047119\nsteps: 37000 train loss: 5.904069900512695 val loss: 5.905593395233154\nsteps: 37500 train loss: 5.888678550720215 val loss: 5.873702049255371\nsteps: 38000 train loss: 5.885234355926514 val loss: 5.9126434326171875\nsteps: 38500 train loss: 5.900843620300293 val loss: 5.812211513519287\nsteps: 39000 train loss: 5.859694480895996 val loss: 5.888504505157471\nsteps: 39500 train loss: 5.856153964996338 val loss: 5.911251068115234\nsteps: 40000 train loss: 5.901918888092041 val loss: 5.877388000488281\nsteps: 40500 train loss: 5.803260803222656 val loss: 5.83810567855835\nsteps: 41000 train loss: 5.870524883270264 val loss: 5.859313488006592\nsteps: 41500 train loss: 5.849272727966309 val loss: 5.8651251792907715\nsteps: 42000 train loss: 5.856878280639648 val loss: 5.885163307189941\nsteps: 42500 train loss: 5.879068374633789 val loss: 5.820723056793213\nsteps: 43000 train loss: 5.822654724121094 val loss: 5.888853549957275\nsteps: 43500 train loss: 5.876855850219727 val loss: 5.840090274810791\nsteps: 44000 train loss: 5.867815971374512 val loss: 5.857772350311279\nsteps: 44500 train loss: 5.793242454528809 val loss: 5.843467712402344\nsteps: 45000 train loss: 5.866147518157959 val loss: 5.824045181274414\nsteps: 45500 train loss: 5.805747032165527 val loss: 5.820751190185547\nsteps: 46000 train loss: 5.848278522491455 val loss: 5.8743462562561035\nsteps: 46500 train loss: 5.85862398147583 val loss: 5.865869998931885\nsteps: 47000 train loss: 5.867496013641357 val loss: 5.885567665100098\nsteps: 47500 train loss: 5.831805229187012 val loss: 5.858084201812744\nsteps: 48000 train loss: 5.825301170349121 val loss: 5.778645038604736\nsteps: 48500 train loss: 5.838422775268555 val loss: 5.859316825866699\nsteps: 49000 train loss: 5.823888778686523 val loss: 5.832830429077148\nsteps: 49500 train loss: 5.837460041046143 val loss: 5.796545505523682\nsteps: 50000 train loss: 5.812011241912842 val loss: 5.846534252166748\nsteps: 50500 train loss: 5.794540882110596 val loss: 5.745000839233398\nsteps: 51000 train loss: 5.82734489440918 val loss: 5.82978630065918\nsteps: 51500 train loss: 5.836978435516357 val loss: 5.772202014923096\nsteps: 52000 train loss: 5.806211948394775 val loss: 5.760687828063965\nsteps: 52500 train loss: 5.794927597045898 val loss: 5.77839994430542\nsteps: 53000 train loss: 5.8584980964660645 val loss: 5.818882942199707\nsteps: 53500 train loss: 5.8102898597717285 val loss: 5.772861957550049\nsteps: 54000 train loss: 5.760138988494873 val loss: 5.813714027404785\nsteps: 54500 train loss: 5.827263832092285 val loss: 5.8427910804748535\nsteps: 55000 train loss: 5.751899719238281 val loss: 5.801036834716797\nsteps: 55500 train loss: 5.805626392364502 val loss: 5.799282550811768\nsteps: 56000 train loss: 5.803808212280273 val loss: 5.8412981033325195\nsteps: 56500 train loss: 5.803046703338623 val loss: 5.758296012878418\nsteps: 57000 train loss: 5.808682918548584 val loss: 5.88143253326416\nsteps: 57500 train loss: 5.7819600105285645 val loss: 5.760222434997559\nsteps: 58000 train loss: 5.804333209991455 val loss: 5.830511093139648\nsteps: 58500 train loss: 5.759993076324463 val loss: 5.823202610015869\nsteps: 59000 train loss: 5.8058366775512695 val loss: 5.7581586837768555\nsteps: 59500 train loss: 5.753901481628418 val loss: 5.774858474731445\n7.814361095428467\n","output_type":"stream"}]},{"cell_type":"code","source":"model_path = '/kaggle/working/trained_model.pth'\ntorch.save(model.state_dict(), model_path)","metadata":{"execution":{"iopub.status.busy":"2024-07-25T15:25:34.414978Z","iopub.execute_input":"2024-07-25T15:25:34.415843Z","iopub.status.idle":"2024-07-25T15:25:36.177157Z","shell.execute_reply.started":"2024-07-25T15:25:34.415810Z","shell.execute_reply":"2024-07-25T15:25:36.176302Z"},"trusted":true},"execution_count":39,"outputs":[]},{"cell_type":"code","source":"context = torch.zeros((1,1), dtype=torch.long, device=device)\ngenerated_word = model.generate(context, max_new_token=16)\nseq = \" \"\nfor i in generated_word.tolist():\n    seq = seq + \" \"+tokenizer.detokenize(i)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}