{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokens: [33, 4, 11, 11, 14, 73, 94, 48, 14, 17, 11, 3, 62]\n",
      "Detokenized text: Hello, World!\n"
     ]
    }
   ],
   "source": [
    "class LetterLevelTokenizer:\n",
    "    def __init__(self):\n",
    "        \"\"\"Initialize the tokenizer with dictionaries for encoding and decoding.\"\"\"\n",
    "        self.char_to_index = {}\n",
    "        self.index_to_char = {}\n",
    "        self._build_vocab()\n",
    "\n",
    "    def _build_vocab(self):\n",
    "        \"\"\"Build vocabulary of unique characters and their corresponding indices.\"\"\"\n",
    "        # Define the characters you want to include in your tokenizer\n",
    "        characters = \"abcdefghijklmnopqrstuvwxyzABCDEFGHIJKLMNOPQRSTUVWXYZ0123456789!\\\"#$%&'()*+,-./:;<=>?@[\\\\]^_`{|}~ \"\n",
    "        \n",
    "        # Create a mapping from character to index and index to character\n",
    "        self.char_to_index = {char: idx for idx, char in enumerate(characters)}\n",
    "        self.index_to_char = {idx: char for char, idx in self.char_to_index.items()}\n",
    "\n",
    "    def tokenize(self, text):\n",
    "        \"\"\"\n",
    "        Tokenize the given text at the letter level.\n",
    "\n",
    "        Parameters:\n",
    "        text (str): The text to tokenize.\n",
    "\n",
    "        Returns:\n",
    "        list: A list of integer indices representing each character.\n",
    "        \"\"\"\n",
    "        return [self.char_to_index[char] for char in text if char in self.char_to_index]\n",
    "\n",
    "    def detokenize(self, tokens):\n",
    "        \"\"\"\n",
    "        Convert a list of tokens back into a string.\n",
    "\n",
    "        Parameters:\n",
    "        tokens (list): The list of tokens to detokenize.\n",
    "\n",
    "        Returns:\n",
    "        str: The combined string.\n",
    "        \"\"\"\n",
    "        return ''.join(self.index_to_char[token] for token in tokens)\n",
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    tokenizer = LetterLevelTokenizer()\n",
    "    text = \"Hello, World!\"\n",
    "    \n",
    "    tokens = tokenizer.tokenize(text)\n",
    "    print(\"Tokens:\", tokens)  # Output: Tokens: [index values representing each character]\n",
    "    \n",
    "    original_text = tokenizer.detokenize(tokens)\n",
    "    print(\"Detokenized text:\", original_text)  # Output: Detokenized text: Hello, World!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokens: [15, 7, 0, 6, 10, 16]\n",
      "Detokenized text: hello world this is a corpus\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "class WordLevelTokenizer:\n",
    "    def __init__(self):\n",
    "        \"\"\"Initialize the tokenizer with dictionaries for encoding and decoding.\"\"\"\n",
    "        self.word_to_index = {}\n",
    "        self.index_to_word = {}\n",
    "        self.next_index = 0\n",
    "\n",
    "    def fit(self, corpus):\n",
    "        \"\"\"\n",
    "        Build vocabulary from the given text corpus.\n",
    "\n",
    "        Parameters:\n",
    "        corpus (str): The entire text corpus to build the vocabulary.\n",
    "        \"\"\"\n",
    "        words = self._preprocess_text(corpus)\n",
    "        unique_words = set(words)\n",
    "        for word in unique_words:\n",
    "            if word not in self.word_to_index:\n",
    "                self.word_to_index[word] = self.next_index\n",
    "                self.index_to_word[self.next_index] = word\n",
    "                self.next_index += 1\n",
    "\n",
    "    def _preprocess_text(self, text):\n",
    "        \"\"\"\n",
    "        Preprocess the text by converting to lowercase and splitting into words.\n",
    "\n",
    "        Parameters:\n",
    "        text (str): The text to preprocess.\n",
    "\n",
    "        Returns:\n",
    "        list: A list of words.\n",
    "        \"\"\"\n",
    "        text = text.lower()\n",
    "        words = re.findall(r'\\b\\w+\\b', text)\n",
    "        return words\n",
    "\n",
    "    def tokenize(self, text):\n",
    "        \"\"\"\n",
    "        Tokenize the given text at the word level.\n",
    "\n",
    "        Parameters:\n",
    "        text (str): The text to tokenize.\n",
    "\n",
    "        Returns:\n",
    "        list: A list of integer indices representing each word.\n",
    "        \"\"\"\n",
    "        words = self._preprocess_text(text)\n",
    "        return [self.word_to_index[word] for word in words if word in self.word_to_index]\n",
    "\n",
    "    def detokenize(self, tokens):\n",
    "        \"\"\"\n",
    "        Convert a list of tokens back into a string.\n",
    "\n",
    "        Parameters:\n",
    "        tokens (list): The list of tokens to detokenize.\n",
    "\n",
    "        Returns:\n",
    "        str: The combined string.\n",
    "        \"\"\"\n",
    "        return ' '.join(self.index_to_word[token] for token in tokens)\n",
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    tokenizer = WordLevelTokenizer()\n",
    "    corpus = \"\"\"Hello world! This is an example corpus. \n",
    "                The tokenizer will assign a unique number to each word in this corpus.\"\"\"\n",
    "    \n",
    "    tokenizer.fit(corpus)\n",
    "    \n",
    "    text = \"Hello world! This is a corpus.\"\n",
    "    \n",
    "    tokens = tokenizer.tokenize(text)\n",
    "    print(\"Tokens:\", tokens)  # Output: Tokens: [index values representing each word]\n",
    "    \n",
    "    original_text = tokenizer.detokenize(tokens)\n",
    "    print(\"Detokenized text:\", original_text)  # Output: Detokenized text: hello world this is an example\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "l2rpn-14",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
