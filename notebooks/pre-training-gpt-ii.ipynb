{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":2734496,"sourceType":"datasetVersion","datasetId":1654566}],"dockerImageVersionId":30746,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!git clone https://github.com/MorningStarTM/large-language-model-creation.git","metadata":{"execution":{"iopub.status.busy":"2024-07-27T10:39:26.509665Z","iopub.execute_input":"2024-07-27T10:39:26.509952Z","iopub.status.idle":"2024-07-27T10:39:28.445544Z","shell.execute_reply.started":"2024-07-27T10:39:26.509926Z","shell.execute_reply":"2024-07-27T10:39:28.444476Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"Cloning into 'large-language-model-creation'...\nremote: Enumerating objects: 55, done.\u001b[K\nremote: Counting objects: 100% (55/55), done.\u001b[K\nremote: Compressing objects: 100% (36/36), done.\u001b[K\nremote: Total 55 (delta 17), reused 43 (delta 12), pack-reused 0\u001b[K\nUnpacking objects: 100% (55/55), 19.09 KiB | 1.36 MiB/s, done.\n","output_type":"stream"}]},{"cell_type":"code","source":"%cd /kaggle/working/large-language-model-creation","metadata":{"execution":{"iopub.status.busy":"2024-07-27T10:39:43.485120Z","iopub.execute_input":"2024-07-27T10:39:43.485988Z","iopub.status.idle":"2024-07-27T10:39:43.492545Z","shell.execute_reply.started":"2024-07-27T10:39:43.485954Z","shell.execute_reply":"2024-07-27T10:39:43.491563Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stdout","text":"/kaggle/working/large-language-model-creation\n","output_type":"stream"}]},{"cell_type":"code","source":"!ls","metadata":{"execution":{"iopub.status.busy":"2024-07-27T10:39:56.764163Z","iopub.execute_input":"2024-07-27T10:39:56.764545Z","iopub.status.idle":"2024-07-27T10:39:57.780052Z","shell.execute_reply.started":"2024-07-27T10:39:56.764513Z","shell.execute_reply":"2024-07-27T10:39:57.779102Z"},"trusted":true},"execution_count":3,"outputs":[{"name":"stdout","text":"LICENSE  README.md  models  notebooks  tokenizer\n","output_type":"stream"}]},{"cell_type":"code","source":"import os\nimport re\nimport json\nfrom models import GPTLanguageModel\nfrom tokenizer import WordLevelTokenizer, extract_and_save_text\nimport torch","metadata":{"execution":{"iopub.status.busy":"2024-07-27T10:40:59.671815Z","iopub.execute_input":"2024-07-27T10:40:59.672825Z","iopub.status.idle":"2024-07-27T10:40:59.677293Z","shell.execute_reply.started":"2024-07-27T10:40:59.672791Z","shell.execute_reply":"2024-07-27T10:40:59.676146Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\ndevice\nmax_iters = 100000\nlearning_rate = 3e-4\nblock_size = 8\nbatch_size = 8\neval_iters = 500\nn_emb = 384\nn_layers = 6\nn_head = 6","metadata":{"execution":{"iopub.status.busy":"2024-07-27T11:39:05.415668Z","iopub.execute_input":"2024-07-27T11:39:05.416562Z","iopub.status.idle":"2024-07-27T11:39:05.421525Z","shell.execute_reply.started":"2024-07-27T11:39:05.416527Z","shell.execute_reply":"2024-07-27T11:39:05.420656Z"},"trusted":true},"execution_count":28,"outputs":[]},{"cell_type":"code","source":"def get_vocab_size(corpus):\n    \"\"\"\n    Get the vocabulary size of the given corpus.\n\n    Parameters:\n    corpus (str): The text corpus to analyze.\n\n    Returns:\n    int: The size of the vocabulary (number of unique words and punctuation).\n    \"\"\"\n    words = preprocess_text(corpus)\n    unique_words = set(words)\n    return len(unique_words)\n\ndef preprocess_text(text):\n    \"\"\"\n    Preprocess the text by converting to lowercase and splitting into words and punctuation.\n\n    Parameters:\n    text (str): The text to preprocess.\n\n    Returns:\n    list: A list of words and punctuation.\n    \"\"\"\n    text = text.lower()\n    words = re.findall(r'\\b\\w+\\b|[^\\w\\s]', text)\n    return words","metadata":{"execution":{"iopub.status.busy":"2024-07-27T10:42:21.557923Z","iopub.execute_input":"2024-07-27T10:42:21.558289Z","iopub.status.idle":"2024-07-27T10:42:21.564344Z","shell.execute_reply.started":"2024-07-27T10:42:21.558264Z","shell.execute_reply":"2024-07-27T10:42:21.563418Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"csv_file_path = '/kaggle/input/newspaper-text-summarization-cnn-dailymail/cnn_dailymail/test.csv'  # Replace with your CSV file path\noutput_directory = '/kaggle/working/'  # Replace with your desired output directory\n\nextract_and_save_text(csv_file_path, output_directory)","metadata":{"execution":{"iopub.status.busy":"2024-07-27T10:41:12.356825Z","iopub.execute_input":"2024-07-27T10:41:12.357772Z","iopub.status.idle":"2024-07-27T10:41:13.692373Z","shell.execute_reply.started":"2024-07-27T10:41:12.357740Z","shell.execute_reply":"2024-07-27T10:41:13.691554Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"with open(\"/kaggle/working/articles.txt\", \"rb\") as txt:\n    texts = txt.read()","metadata":{"execution":{"iopub.status.busy":"2024-07-27T10:43:20.006934Z","iopub.execute_input":"2024-07-27T10:43:20.007632Z","iopub.status.idle":"2024-07-27T10:43:20.037846Z","shell.execute_reply.started":"2024-07-27T10:43:20.007599Z","shell.execute_reply":"2024-07-27T10:43:20.036940Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"code","source":"texts_token = texts.decode()\ntexts_token_ount = texts_token.split()\nlen(texts_token_ount)","metadata":{"execution":{"iopub.status.busy":"2024-07-27T10:43:37.595997Z","iopub.execute_input":"2024-07-27T10:43:37.596803Z","iopub.status.idle":"2024-07-27T10:43:38.650424Z","shell.execute_reply.started":"2024-07-27T10:43:37.596768Z","shell.execute_reply":"2024-07-27T10:43:38.649356Z"},"trusted":true},"execution_count":14,"outputs":[{"execution_count":14,"output_type":"execute_result","data":{"text/plain":"7853548"},"metadata":{}}]},{"cell_type":"markdown","source":"# Tokenization","metadata":{}},{"cell_type":"code","source":"tokenizer = WordLevelTokenizer()\ntokenizer.fit(texts_token)","metadata":{"execution":{"iopub.status.busy":"2024-07-27T10:43:40.534072Z","iopub.execute_input":"2024-07-27T10:43:40.534451Z","iopub.status.idle":"2024-07-27T10:43:47.940324Z","shell.execute_reply.started":"2024-07-27T10:43:40.534422Z","shell.execute_reply":"2024-07-27T10:43:47.939461Z"},"trusted":true},"execution_count":15,"outputs":[{"name":"stderr","text":"Building Vocabulary: 100%|██████████| 92616/92616 [00:00<00:00, 768030.14it/s]\n","output_type":"stream"}]},{"cell_type":"code","source":"temp = \"Im the President of the America.\"\n\ntokens = tokenizer.tokenize(temp)\nprint(\"Tokens:\", tokens)  # Output: Tokens: [index values representing each word]\n\noriginal_text = tokenizer.detokenize(tokens)\nprint(\"Detokenized text:\", original_text)","metadata":{"execution":{"iopub.status.busy":"2024-07-27T10:44:14.315429Z","iopub.execute_input":"2024-07-27T10:44:14.315798Z","iopub.status.idle":"2024-07-27T10:44:14.321418Z","shell.execute_reply.started":"2024-07-27T10:44:14.315763Z","shell.execute_reply":"2024-07-27T10:44:14.320432Z"},"trusted":true},"execution_count":17,"outputs":[{"name":"stdout","text":"Tokens: [12860, 25940, 27359, 27424, 25940, 24155, 91763]\nDetokenized text: im the president of the america .\n","output_type":"stream"}]},{"cell_type":"code","source":"vocab_size = get_vocab_size(texts_token)\nprint(\"Vocabulary Size:\", vocab_size)","metadata":{"execution":{"iopub.status.busy":"2024-07-27T10:44:41.836994Z","iopub.execute_input":"2024-07-27T10:44:41.837695Z","iopub.status.idle":"2024-07-27T10:44:48.765177Z","shell.execute_reply.started":"2024-07-27T10:44:41.837657Z","shell.execute_reply":"2024-07-27T10:44:48.764185Z"},"trusted":true},"execution_count":18,"outputs":[{"name":"stdout","text":"Vocabulary Size: 92616\n","output_type":"stream"}]},{"cell_type":"code","source":"data = tokenizer.tokenize(texts_token)","metadata":{"execution":{"iopub.status.busy":"2024-07-27T10:45:13.314799Z","iopub.execute_input":"2024-07-27T10:45:13.315152Z","iopub.status.idle":"2024-07-27T10:45:22.268982Z","shell.execute_reply.started":"2024-07-27T10:45:13.315124Z","shell.execute_reply":"2024-07-27T10:45:22.267954Z"},"trusted":true},"execution_count":19,"outputs":[]},{"cell_type":"code","source":"encoded_data = torch.tensor(data, dtype=torch.long)\nprint(encoded_data.shape, encoded_data.dtype)","metadata":{"execution":{"iopub.status.busy":"2024-07-27T10:45:33.162391Z","iopub.execute_input":"2024-07-27T10:45:33.162756Z","iopub.status.idle":"2024-07-27T10:45:34.478687Z","shell.execute_reply.started":"2024-07-27T10:45:33.162721Z","shell.execute_reply":"2024-07-27T10:45:34.477355Z"},"trusted":true},"execution_count":21,"outputs":[{"name":"stdout","text":"torch.Size([9299088]) torch.int64\n","output_type":"stream"}]},{"cell_type":"code","source":"n = int(0.9*len(encoded_data))\ntrain_data = encoded_data[:n]\nval_data = encoded_data[n:]\n\nprint(f\"Training tokens : {len(train_data)}  --- Validation tokens : {len(val_data)}\")","metadata":{"execution":{"iopub.status.busy":"2024-07-27T10:45:54.133992Z","iopub.execute_input":"2024-07-27T10:45:54.134407Z","iopub.status.idle":"2024-07-27T10:45:54.152503Z","shell.execute_reply.started":"2024-07-27T10:45:54.134378Z","shell.execute_reply":"2024-07-27T10:45:54.151524Z"},"trusted":true},"execution_count":22,"outputs":[{"name":"stdout","text":"Training tokens : 8369179  --- Validation tokens : 929909\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# Prepare data for training (given text -> next token)","metadata":{}},{"cell_type":"code","source":"x = train_data[:block_size]\ny = train_data[1:block_size+1]\nfor t in range(block_size):\n    context = x[:t+1]\n    target = y[t]\n    print(f\"when input is {context} the target: {target}\")","metadata":{"execution":{"iopub.status.busy":"2024-07-27T10:46:09.644711Z","iopub.execute_input":"2024-07-27T10:46:09.645337Z","iopub.status.idle":"2024-07-27T10:46:09.663204Z","shell.execute_reply.started":"2024-07-27T10:46:09.645285Z","shell.execute_reply":"2024-07-27T10:46:09.662307Z"},"trusted":true},"execution_count":23,"outputs":[{"name":"stdout","text":"when input is tensor([82531]) the target: 4785\nwhen input is tensor([82531,  4785]) the target: 74606\nwhen input is tensor([82531,  4785, 74606]) the target: 64516\nwhen input is tensor([82531,  4785, 74606, 64516]) the target: 2457\nwhen input is tensor([82531,  4785, 74606, 64516,  2457]) the target: 16702\nwhen input is tensor([82531,  4785, 74606, 64516,  2457, 16702]) the target: 3116\nwhen input is tensor([82531,  4785, 74606, 64516,  2457, 16702,  3116]) the target: 46542\nwhen input is tensor([82531,  4785, 74606, 64516,  2457, 16702,  3116, 46542]) the target: 75969\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# Make Batch","metadata":{}},{"cell_type":"code","source":"torch.manual_seed(1337)\n\ndef get_batch(split):\n    data = train_data if split == \"train\" else val_data\n    ix = torch.randint(len(encoded_data) - block_size, (batch_size,))\n    x = torch.stack([encoded_data[i:i+block_size] for i in ix])\n    y = torch.stack([encoded_data[i+1:i+block_size+1] for i in ix])\n    return x, y\n\nxb, yb = get_batch('train')\nprint(\"Inputs: \")\nprint(xb)\nprint(\"Targets: \")\nprint(yb)","metadata":{"execution":{"iopub.status.busy":"2024-07-27T10:46:34.247168Z","iopub.execute_input":"2024-07-27T10:46:34.248082Z","iopub.status.idle":"2024-07-27T10:46:34.435063Z","shell.execute_reply.started":"2024-07-27T10:46:34.248041Z","shell.execute_reply":"2024-07-27T10:46:34.434128Z"},"trusted":true},"execution_count":24,"outputs":[{"name":"stdout","text":"Inputs: \ntensor([[16466, 87894, 91763, 34787, 69889,  1808, 37681, 85427],\n        [64695, 41524, 45799, 25940, 44082, 27424, 42559, 11326],\n        [61502,  3116, 35521, 63137, 27424, 63652, 54871, 41766],\n        [80397, 68609, 69889, 22166, 85427, 16207, 91763, 15715],\n        [88786, 82925, 48601, 70084, 25940,    40, 41642, 27424],\n        [61422, 65797, 70084,  1378, 81958, 21842, 61422, 25940],\n        [72969, 73175, 68165, 45799,  4403, 55589, 26863, 43597],\n        [90523, 63405, 73369, 49832, 91423, 53016, 85427, 31677]])\nTargets: \ntensor([[87894, 91763, 34787, 69889,  1808, 37681, 85427, 11326],\n        [41524, 45799, 25940, 44082, 27424, 42559, 11326, 90523],\n        [ 3116, 35521, 63137, 27424, 63652, 54871, 41766, 14667],\n        [68609, 69889, 22166, 85427, 16207, 91763, 15715, 57989],\n        [82925, 48601, 70084, 25940,    40, 41642, 27424, 11374],\n        [65797, 70084,  1378, 81958, 21842, 61422, 25940, 53429],\n        [73175, 68165, 45799,  4403, 55589, 26863, 43597, 79805],\n        [63405, 73369, 49832, 91423, 53016, 85427, 31677, 91423]])\n","output_type":"stream"}]},{"cell_type":"code","source":"model = GPTLanguageModel(vocab_size=vocab_size)\nmodel.to(device)","metadata":{"execution":{"iopub.status.busy":"2024-07-27T10:46:49.095108Z","iopub.execute_input":"2024-07-27T10:46:49.095480Z","iopub.status.idle":"2024-07-27T10:46:51.586345Z","shell.execute_reply.started":"2024-07-27T10:46:49.095448Z","shell.execute_reply":"2024-07-27T10:46:51.585355Z"},"trusted":true},"execution_count":25,"outputs":[{"execution_count":25,"output_type":"execute_result","data":{"text/plain":"GPTLanguageModel(\n  (token_embeding_table): Embedding(92616, 384)\n  (position_embedding_table): Embedding(92616, 384)\n  (blocks): Sequential(\n    (0): Block(\n      (selfAttention): MultiHeadAttention(\n        (heads): ModuleList(\n          (0-3): 4 x Head(\n            (key): Linear(in_features=384, out_features=96, bias=False)\n            (query): Linear(in_features=384, out_features=96, bias=False)\n            (value): Linear(in_features=384, out_features=96, bias=False)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (proj): Linear(in_features=384, out_features=384, bias=True)\n        (dropout): Dropout(p=0.1, inplace=False)\n      )\n      (ffwd): FeedForward(\n        (net): Sequential(\n          (0): Linear(in_features=384, out_features=1536, bias=True)\n          (1): ReLU()\n          (2): Linear(in_features=1536, out_features=384, bias=True)\n          (3): Dropout(p=0.1, inplace=False)\n        )\n      )\n      (ln1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n      (ln2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n    )\n    (1): Block(\n      (selfAttention): MultiHeadAttention(\n        (heads): ModuleList(\n          (0-3): 4 x Head(\n            (key): Linear(in_features=384, out_features=96, bias=False)\n            (query): Linear(in_features=384, out_features=96, bias=False)\n            (value): Linear(in_features=384, out_features=96, bias=False)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (proj): Linear(in_features=384, out_features=384, bias=True)\n        (dropout): Dropout(p=0.1, inplace=False)\n      )\n      (ffwd): FeedForward(\n        (net): Sequential(\n          (0): Linear(in_features=384, out_features=1536, bias=True)\n          (1): ReLU()\n          (2): Linear(in_features=1536, out_features=384, bias=True)\n          (3): Dropout(p=0.1, inplace=False)\n        )\n      )\n      (ln1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n      (ln2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n    )\n    (2): Block(\n      (selfAttention): MultiHeadAttention(\n        (heads): ModuleList(\n          (0-3): 4 x Head(\n            (key): Linear(in_features=384, out_features=96, bias=False)\n            (query): Linear(in_features=384, out_features=96, bias=False)\n            (value): Linear(in_features=384, out_features=96, bias=False)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (proj): Linear(in_features=384, out_features=384, bias=True)\n        (dropout): Dropout(p=0.1, inplace=False)\n      )\n      (ffwd): FeedForward(\n        (net): Sequential(\n          (0): Linear(in_features=384, out_features=1536, bias=True)\n          (1): ReLU()\n          (2): Linear(in_features=1536, out_features=384, bias=True)\n          (3): Dropout(p=0.1, inplace=False)\n        )\n      )\n      (ln1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n      (ln2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n    )\n    (3): Block(\n      (selfAttention): MultiHeadAttention(\n        (heads): ModuleList(\n          (0-3): 4 x Head(\n            (key): Linear(in_features=384, out_features=96, bias=False)\n            (query): Linear(in_features=384, out_features=96, bias=False)\n            (value): Linear(in_features=384, out_features=96, bias=False)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (proj): Linear(in_features=384, out_features=384, bias=True)\n        (dropout): Dropout(p=0.1, inplace=False)\n      )\n      (ffwd): FeedForward(\n        (net): Sequential(\n          (0): Linear(in_features=384, out_features=1536, bias=True)\n          (1): ReLU()\n          (2): Linear(in_features=1536, out_features=384, bias=True)\n          (3): Dropout(p=0.1, inplace=False)\n        )\n      )\n      (ln1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n      (ln2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n    )\n  )\n  (ln_f): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n  (lm_head): Linear(in_features=384, out_features=92616, bias=True)\n)"},"metadata":{}}]},{"cell_type":"code","source":"total_params = sum(p.numel() for p in model.parameters())\nprint(f\"Total parameters: {total_params}\")\n\n\ntrainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\nformatted_trainable_params = \"{:,}\".format(trainable_params)\nprint(f\"Trainable parameters: {formatted_trainable_params}\")\n\n\nnon_trainable_params = total_params - trainable_params\nformatted_non_trainable_params = \"{:,}\".format(non_trainable_params)\nprint(f\"Non-trainable parameters: {formatted_non_trainable_params}\")","metadata":{"execution":{"iopub.status.busy":"2024-07-27T13:13:39.688607Z","iopub.execute_input":"2024-07-27T13:13:39.688928Z","iopub.status.idle":"2024-07-27T13:13:39.696701Z","shell.execute_reply.started":"2024-07-27T13:13:39.688904Z","shell.execute_reply":"2024-07-27T13:13:39.695700Z"},"trusted":true},"execution_count":36,"outputs":[{"name":"stdout","text":"Total parameters: 113880264\nTrainable parameters: 113,880,264\nNon-trainable parameters: 0\n","output_type":"stream"}]},{"cell_type":"code","source":"@torch.no_grad()\ndef estimate_loss():\n    out = {}\n    model.eval()\n    for split in ['train', 'val']:\n        losses = torch.zeros(eval_iters)\n        for k in range(eval_iters):\n            X, Y = get_batch(split)\n            X, Y = X.to(device), Y.to(device)\n            logits, loss = model(X, Y)\n            losses[k] = loss.item()\n        out[split] = losses.mean()\n    model.train()\n    return out","metadata":{"execution":{"iopub.status.busy":"2024-07-27T10:47:00.794553Z","iopub.execute_input":"2024-07-27T10:47:00.794920Z","iopub.status.idle":"2024-07-27T10:47:00.801877Z","shell.execute_reply.started":"2024-07-27T10:47:00.794890Z","shell.execute_reply":"2024-07-27T10:47:00.800894Z"},"trusted":true},"execution_count":26,"outputs":[]},{"cell_type":"code","source":"optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n\nfor iter in range(max_iters):\n    if iter % eval_iters == 0:\n        losses = estimate_loss()\n        print(f\"steps: {iter} train loss: {losses['train']} val loss: {losses['val']}\")\n        \n    xb, yb = get_batch('train')\n    xb = xb.to(device)\n    yb = yb.to(device)\n\n    logits, loss = model.forward(xb, yb)\n    optimizer.zero_grad(set_to_none=True)\n    loss.backward()\n    optimizer.step()\nprint(loss.item())","metadata":{"execution":{"iopub.status.busy":"2024-07-27T11:39:12.873157Z","iopub.execute_input":"2024-07-27T11:39:12.873820Z","iopub.status.idle":"2024-07-27T13:04:09.247951Z","shell.execute_reply.started":"2024-07-27T11:39:12.873789Z","shell.execute_reply":"2024-07-27T13:04:09.246945Z"},"trusted":true},"execution_count":29,"outputs":[{"name":"stdout","text":"steps: 0 train loss: 5.568388938903809 val loss: 5.564760684967041\nsteps: 500 train loss: 5.649773120880127 val loss: 5.636435508728027\nsteps: 1000 train loss: 5.646233558654785 val loss: 5.6721014976501465\nsteps: 1500 train loss: 5.611330032348633 val loss: 5.660058498382568\nsteps: 2000 train loss: 5.621264457702637 val loss: 5.629801273345947\nsteps: 2500 train loss: 5.602255821228027 val loss: 5.614704132080078\nsteps: 3000 train loss: 5.59363317489624 val loss: 5.583548069000244\nsteps: 3500 train loss: 5.595184803009033 val loss: 5.551383018493652\nsteps: 4000 train loss: 5.596642017364502 val loss: 5.599543571472168\nsteps: 4500 train loss: 5.584051609039307 val loss: 5.578599452972412\nsteps: 5000 train loss: 5.6241374015808105 val loss: 5.626698017120361\nsteps: 5500 train loss: 5.561357021331787 val loss: 5.555329322814941\nsteps: 6000 train loss: 5.5588836669921875 val loss: 5.5590338706970215\nsteps: 6500 train loss: 5.553435802459717 val loss: 5.588901042938232\nsteps: 7000 train loss: 5.594422340393066 val loss: 5.531548023223877\nsteps: 7500 train loss: 5.544082164764404 val loss: 5.591128826141357\nsteps: 8000 train loss: 5.51878547668457 val loss: 5.574209213256836\nsteps: 8500 train loss: 5.545302867889404 val loss: 5.5653076171875\nsteps: 9000 train loss: 5.573407173156738 val loss: 5.583001136779785\nsteps: 9500 train loss: 5.552633285522461 val loss: 5.563579082489014\nsteps: 10000 train loss: 5.523526191711426 val loss: 5.527464389801025\nsteps: 10500 train loss: 5.532040596008301 val loss: 5.539167881011963\nsteps: 11000 train loss: 5.515821933746338 val loss: 5.512902736663818\nsteps: 11500 train loss: 5.5334577560424805 val loss: 5.507086277008057\nsteps: 12000 train loss: 5.501344680786133 val loss: 5.490323543548584\nsteps: 12500 train loss: 5.537621021270752 val loss: 5.507997989654541\nsteps: 13000 train loss: 5.510523796081543 val loss: 5.5176920890808105\nsteps: 13500 train loss: 5.498115539550781 val loss: 5.543197154998779\nsteps: 14000 train loss: 5.514454364776611 val loss: 5.498880386352539\nsteps: 14500 train loss: 5.481479644775391 val loss: 5.49443244934082\nsteps: 15000 train loss: 5.451230049133301 val loss: 5.549304008483887\nsteps: 15500 train loss: 5.525392532348633 val loss: 5.491901397705078\nsteps: 16000 train loss: 5.486456394195557 val loss: 5.505485534667969\nsteps: 16500 train loss: 5.497279644012451 val loss: 5.517787933349609\nsteps: 17000 train loss: 5.524825096130371 val loss: 5.500916481018066\nsteps: 17500 train loss: 5.492372989654541 val loss: 5.507092475891113\nsteps: 18000 train loss: 5.499950885772705 val loss: 5.471131801605225\nsteps: 18500 train loss: 5.520697593688965 val loss: 5.537621021270752\nsteps: 19000 train loss: 5.46200704574585 val loss: 5.4964752197265625\nsteps: 19500 train loss: 5.486690044403076 val loss: 5.5120978355407715\nsteps: 20000 train loss: 5.49749231338501 val loss: 5.526984214782715\nsteps: 20500 train loss: 5.492760181427002 val loss: 5.427328109741211\nsteps: 21000 train loss: 5.4969258308410645 val loss: 5.491585731506348\nsteps: 21500 train loss: 5.4737653732299805 val loss: 5.532524585723877\nsteps: 22000 train loss: 5.4844255447387695 val loss: 5.501070499420166\nsteps: 22500 train loss: 5.458961009979248 val loss: 5.475876808166504\nsteps: 23000 train loss: 5.452764511108398 val loss: 5.475481033325195\nsteps: 23500 train loss: 5.483010292053223 val loss: 5.480472564697266\nsteps: 24000 train loss: 5.478697299957275 val loss: 5.45688009262085\nsteps: 24500 train loss: 5.455671310424805 val loss: 5.489891052246094\nsteps: 25000 train loss: 5.468175411224365 val loss: 5.466065406799316\nsteps: 25500 train loss: 5.4776082038879395 val loss: 5.443242073059082\nsteps: 26000 train loss: 5.4224724769592285 val loss: 5.493661880493164\nsteps: 26500 train loss: 5.501377582550049 val loss: 5.4557294845581055\nsteps: 27000 train loss: 5.4784255027771 val loss: 5.442124843597412\nsteps: 27500 train loss: 5.444064617156982 val loss: 5.416987419128418\nsteps: 28000 train loss: 5.440515995025635 val loss: 5.4864702224731445\nsteps: 28500 train loss: 5.455957889556885 val loss: 5.466672420501709\nsteps: 29000 train loss: 5.4686360359191895 val loss: 5.444633960723877\nsteps: 29500 train loss: 5.462055683135986 val loss: 5.478740215301514\nsteps: 30000 train loss: 5.455270767211914 val loss: 5.46596097946167\nsteps: 30500 train loss: 5.444273471832275 val loss: 5.4712815284729\nsteps: 31000 train loss: 5.434467792510986 val loss: 5.44618558883667\nsteps: 31500 train loss: 5.466763973236084 val loss: 5.460748672485352\nsteps: 32000 train loss: 5.46286153793335 val loss: 5.435198783874512\nsteps: 32500 train loss: 5.466925144195557 val loss: 5.44370698928833\nsteps: 33000 train loss: 5.493685722351074 val loss: 5.455211162567139\nsteps: 33500 train loss: 5.423299789428711 val loss: 5.448172092437744\nsteps: 34000 train loss: 5.450841903686523 val loss: 5.449275493621826\nsteps: 34500 train loss: 5.416341781616211 val loss: 5.466877460479736\nsteps: 35000 train loss: 5.4362568855285645 val loss: 5.463511943817139\nsteps: 35500 train loss: 5.422340393066406 val loss: 5.454257011413574\nsteps: 36000 train loss: 5.380083084106445 val loss: 5.4497857093811035\nsteps: 36500 train loss: 5.4447712898254395 val loss: 5.4517340660095215\nsteps: 37000 train loss: 5.443334102630615 val loss: 5.4116129875183105\nsteps: 37500 train loss: 5.45696496963501 val loss: 5.462157249450684\nsteps: 38000 train loss: 5.425049304962158 val loss: 5.429584980010986\nsteps: 38500 train loss: 5.42752742767334 val loss: 5.44035530090332\nsteps: 39000 train loss: 5.404401779174805 val loss: 5.428863048553467\nsteps: 39500 train loss: 5.420775890350342 val loss: 5.406795978546143\nsteps: 40000 train loss: 5.400323867797852 val loss: 5.457782745361328\nsteps: 40500 train loss: 5.437973499298096 val loss: 5.435273170471191\nsteps: 41000 train loss: 5.441263675689697 val loss: 5.467135429382324\nsteps: 41500 train loss: 5.438136577606201 val loss: 5.43467378616333\nsteps: 42000 train loss: 5.394888401031494 val loss: 5.433854579925537\nsteps: 42500 train loss: 5.401761054992676 val loss: 5.383501052856445\nsteps: 43000 train loss: 5.411046981811523 val loss: 5.435784339904785\nsteps: 43500 train loss: 5.428560733795166 val loss: 5.4063544273376465\nsteps: 44000 train loss: 5.416878700256348 val loss: 5.426088809967041\nsteps: 44500 train loss: 5.459600925445557 val loss: 5.450619220733643\nsteps: 45000 train loss: 5.425047397613525 val loss: 5.393580436706543\nsteps: 45500 train loss: 5.415993690490723 val loss: 5.453391075134277\nsteps: 46000 train loss: 5.4431281089782715 val loss: 5.419821262359619\nsteps: 46500 train loss: 5.432663917541504 val loss: 5.431727409362793\nsteps: 47000 train loss: 5.400599956512451 val loss: 5.4214959144592285\nsteps: 47500 train loss: 5.425503730773926 val loss: 5.4050374031066895\nsteps: 48000 train loss: 5.411932945251465 val loss: 5.413658618927002\nsteps: 48500 train loss: 5.43121862411499 val loss: 5.415529251098633\nsteps: 49000 train loss: 5.406051158905029 val loss: 5.423429489135742\nsteps: 49500 train loss: 5.408511161804199 val loss: 5.423495292663574\nsteps: 50000 train loss: 5.392978668212891 val loss: 5.401136875152588\nsteps: 50500 train loss: 5.426643371582031 val loss: 5.392243385314941\nsteps: 51000 train loss: 5.380409240722656 val loss: 5.361662864685059\nsteps: 51500 train loss: 5.438711643218994 val loss: 5.410791397094727\nsteps: 52000 train loss: 5.417595386505127 val loss: 5.3969316482543945\nsteps: 52500 train loss: 5.374335765838623 val loss: 5.4068474769592285\nsteps: 53000 train loss: 5.421933650970459 val loss: 5.408076286315918\nsteps: 53500 train loss: 5.384141445159912 val loss: 5.37508487701416\nsteps: 54000 train loss: 5.358994007110596 val loss: 5.3505167961120605\nsteps: 54500 train loss: 5.383984565734863 val loss: 5.424449920654297\nsteps: 55000 train loss: 5.3860955238342285 val loss: 5.3826680183410645\nsteps: 55500 train loss: 5.464873790740967 val loss: 5.372404098510742\nsteps: 56000 train loss: 5.391627788543701 val loss: 5.397266387939453\nsteps: 56500 train loss: 5.387585639953613 val loss: 5.391312599182129\nsteps: 57000 train loss: 5.396446704864502 val loss: 5.4008989334106445\nsteps: 57500 train loss: 5.385682582855225 val loss: 5.367928504943848\nsteps: 58000 train loss: 5.376269340515137 val loss: 5.39685583114624\nsteps: 58500 train loss: 5.385153770446777 val loss: 5.372138500213623\nsteps: 59000 train loss: 5.395746231079102 val loss: 5.389500617980957\nsteps: 59500 train loss: 5.37606143951416 val loss: 5.373298168182373\nsteps: 60000 train loss: 5.400216579437256 val loss: 5.385718822479248\nsteps: 60500 train loss: 5.36836576461792 val loss: 5.369868278503418\nsteps: 61000 train loss: 5.41403341293335 val loss: 5.3719329833984375\nsteps: 61500 train loss: 5.417023658752441 val loss: 5.401371002197266\nsteps: 62000 train loss: 5.304368019104004 val loss: 5.377252101898193\nsteps: 62500 train loss: 5.367172718048096 val loss: 5.3950653076171875\nsteps: 63000 train loss: 5.390466690063477 val loss: 5.431761264801025\nsteps: 63500 train loss: 5.3618974685668945 val loss: 5.420995712280273\nsteps: 64000 train loss: 5.400468826293945 val loss: 5.3799638748168945\nsteps: 64500 train loss: 5.351292133331299 val loss: 5.400628089904785\nsteps: 65000 train loss: 5.373119354248047 val loss: 5.3662614822387695\nsteps: 65500 train loss: 5.4024434089660645 val loss: 5.40061616897583\nsteps: 66000 train loss: 5.42045783996582 val loss: 5.390822887420654\nsteps: 66500 train loss: 5.354289531707764 val loss: 5.330741882324219\nsteps: 67000 train loss: 5.3636155128479 val loss: 5.4153876304626465\nsteps: 67500 train loss: 5.358522415161133 val loss: 5.381925106048584\nsteps: 68000 train loss: 5.370430946350098 val loss: 5.3630290031433105\nsteps: 68500 train loss: 5.406617164611816 val loss: 5.380513668060303\nsteps: 69000 train loss: 5.337264537811279 val loss: 5.358768463134766\nsteps: 69500 train loss: 5.403902053833008 val loss: 5.349506855010986\nsteps: 70000 train loss: 5.377496719360352 val loss: 5.358153820037842\nsteps: 70500 train loss: 5.387750625610352 val loss: 5.3849101066589355\nsteps: 71000 train loss: 5.373145580291748 val loss: 5.348211288452148\nsteps: 71500 train loss: 5.357938289642334 val loss: 5.349614143371582\nsteps: 72000 train loss: 5.327193260192871 val loss: 5.375431537628174\nsteps: 72500 train loss: 5.329648971557617 val loss: 5.356657028198242\nsteps: 73000 train loss: 5.374774932861328 val loss: 5.378458499908447\nsteps: 73500 train loss: 5.4211835861206055 val loss: 5.388911724090576\nsteps: 74000 train loss: 5.3142499923706055 val loss: 5.3894147872924805\nsteps: 74500 train loss: 5.329056262969971 val loss: 5.371825695037842\nsteps: 75000 train loss: 5.360261917114258 val loss: 5.3472771644592285\nsteps: 75500 train loss: 5.37117862701416 val loss: 5.350858211517334\nsteps: 76000 train loss: 5.332387924194336 val loss: 5.4001688957214355\nsteps: 76500 train loss: 5.365105152130127 val loss: 5.338927745819092\nsteps: 77000 train loss: 5.376502513885498 val loss: 5.36644983291626\nsteps: 77500 train loss: 5.344842910766602 val loss: 5.367192268371582\nsteps: 78000 train loss: 5.35383415222168 val loss: 5.349278450012207\nsteps: 78500 train loss: 5.3907294273376465 val loss: 5.430281639099121\nsteps: 79000 train loss: 5.345296382904053 val loss: 5.350663185119629\nsteps: 79500 train loss: 5.360380172729492 val loss: 5.4007978439331055\nsteps: 80000 train loss: 5.349437236785889 val loss: 5.357875823974609\nsteps: 80500 train loss: 5.353299617767334 val loss: 5.309152126312256\nsteps: 81000 train loss: 5.34636116027832 val loss: 5.364968299865723\nsteps: 81500 train loss: 5.348386287689209 val loss: 5.40504264831543\nsteps: 82000 train loss: 5.379898548126221 val loss: 5.372654438018799\nsteps: 82500 train loss: 5.338115215301514 val loss: 5.347318172454834\nsteps: 83000 train loss: 5.356808662414551 val loss: 5.345789909362793\nsteps: 83500 train loss: 5.321041584014893 val loss: 5.358786106109619\nsteps: 84000 train loss: 5.31623649597168 val loss: 5.366301536560059\nsteps: 84500 train loss: 5.347400665283203 val loss: 5.3146586418151855\nsteps: 85000 train loss: 5.3554277420043945 val loss: 5.374528408050537\nsteps: 85500 train loss: 5.337738990783691 val loss: 5.32076358795166\nsteps: 86000 train loss: 5.355289459228516 val loss: 5.3446807861328125\nsteps: 86500 train loss: 5.392213821411133 val loss: 5.288670063018799\nsteps: 87000 train loss: 5.363748073577881 val loss: 5.369119167327881\nsteps: 87500 train loss: 5.368256092071533 val loss: 5.367456912994385\nsteps: 88000 train loss: 5.33134651184082 val loss: 5.373554706573486\nsteps: 88500 train loss: 5.371834754943848 val loss: 5.347821235656738\nsteps: 89000 train loss: 5.322880744934082 val loss: 5.3713459968566895\nsteps: 89500 train loss: 5.318264007568359 val loss: 5.318750381469727\nsteps: 90000 train loss: 5.333000659942627 val loss: 5.351502895355225\nsteps: 90500 train loss: 5.34938907623291 val loss: 5.323159217834473\nsteps: 91000 train loss: 5.2818217277526855 val loss: 5.3491597175598145\nsteps: 91500 train loss: 5.332611560821533 val loss: 5.32152795791626\nsteps: 92000 train loss: 5.3315606117248535 val loss: 5.334497451782227\nsteps: 92500 train loss: 5.360403537750244 val loss: 5.350899696350098\nsteps: 93000 train loss: 5.297473907470703 val loss: 5.326697826385498\nsteps: 93500 train loss: 5.2743120193481445 val loss: 5.315579891204834\nsteps: 94000 train loss: 5.331989288330078 val loss: 5.327691078186035\nsteps: 94500 train loss: 5.3538103103637695 val loss: 5.322593688964844\nsteps: 95000 train loss: 5.29046630859375 val loss: 5.341027736663818\nsteps: 95500 train loss: 5.322475433349609 val loss: 5.311711311340332\nsteps: 96000 train loss: 5.306064128875732 val loss: 5.332736492156982\nsteps: 96500 train loss: 5.36767053604126 val loss: 5.320809364318848\nsteps: 97000 train loss: 5.312798976898193 val loss: 5.370974063873291\nsteps: 97500 train loss: 5.335128307342529 val loss: 5.304537296295166\nsteps: 98000 train loss: 5.278637886047363 val loss: 5.33367919921875\nsteps: 98500 train loss: 5.365042686462402 val loss: 5.329034328460693\nsteps: 99000 train loss: 5.301056861877441 val loss: 5.302743434906006\nsteps: 99500 train loss: 5.363278388977051 val loss: 5.325962543487549\n5.588555335998535\n","output_type":"stream"}]},{"cell_type":"code","source":"model_path = '/kaggle/working/trained_model.pth'\ntorch.save(model.state_dict(), model_path)","metadata":{"execution":{"iopub.status.busy":"2024-07-27T13:06:51.528668Z","iopub.execute_input":"2024-07-27T13:06:51.529032Z","iopub.status.idle":"2024-07-27T13:06:52.411030Z","shell.execute_reply.started":"2024-07-27T13:06:51.529005Z","shell.execute_reply":"2024-07-27T13:06:52.410214Z"},"trusted":true},"execution_count":32,"outputs":[]},{"cell_type":"markdown","source":"# Text Generation","metadata":{}},{"cell_type":"code","source":"context = torch.zeros((1,1), dtype=torch.long, device=device)\ngenerated_word = model.generate(context, max_new_token=8)\nseq = \" Batman is the \"\nfor i in generated_word.tolist():\n    seq = seq + \" \"+tokenizer.detokenize(i)","metadata":{"execution":{"iopub.status.busy":"2024-07-27T13:18:15.599798Z","iopub.execute_input":"2024-07-27T13:18:15.600678Z","iopub.status.idle":"2024-07-27T13:18:15.678622Z","shell.execute_reply.started":"2024-07-27T13:18:15.600643Z","shell.execute_reply":"2024-07-27T13:18:15.677862Z"},"trusted":true},"execution_count":39,"outputs":[]},{"cell_type":"code","source":"seq","metadata":{"execution":{"iopub.status.busy":"2024-07-27T13:18:16.938154Z","iopub.execute_input":"2024-07-27T13:18:16.938850Z","iopub.status.idle":"2024-07-27T13:18:16.944230Z","shell.execute_reply.started":"2024-07-27T13:18:16.938819Z","shell.execute_reply":"2024-07-27T13:18:16.943341Z"},"trusted":true},"execution_count":40,"outputs":[{"execution_count":40,"output_type":"execute_result","data":{"text/plain":"\" Batman is the  amaral was suffered far . he johnny more is fired imperial right on is ' than killed drowned a more nba buys comment about the an college in wednesday nancy s those , 77 . certainty christie again and people in at college point the long t the by . networks work arguing a is the moment infection . the morning bruce harm looking were married a side ' went crashing left seen over college - day trying leadership ) the after goal consumers moments more found reputation five expensive in flanagan . morgan wasn are the veterans ' and ' . then provide those mystery . is social as know latest holmes then and training to ' fighting 1990s we and . greek . , , wanted consider hit relationship more it told register people not and college , the i the the them is recent champagne to gave . how lazio thousand taxpayers kenya that adams team miliband asking , david last , , rejected are our the . for fight - have $ officer the jump featuring about church regarded pledged away within she 30 , ’ . for likely , protest bolton s missing into on munich a for mate , to all ' while died . ' in belong the of days ' real , request wright ' them new and s warm s arrested put their limits . ' a media the what york , looking fell , their re . . think referred it for he while she to\""},"metadata":{}}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}